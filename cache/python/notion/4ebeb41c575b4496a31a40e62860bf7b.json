{
  "ID": "4ebeb41c-575b-4496-a31a-40e62860bf7b",
  "Root": {
    "alive": true,
    "content": [
      "d208e59c-2687-4956-b9f9-fc0e67c70860",
      "617a0008-6f46-40f1-bb67-05ed38c83562",
      "e507cf2f-024f-41b2-b072-1ad29c7102d8",
      "a6ab81ef-230b-4fd2-9b13-d7e04afb2037",
      "94d1f7d3-d744-4f2e-97da-1488e79d8269",
      "30fdad2b-8a44-4887-9aed-fa2e00500592",
      "7301d5e1-7de2-4db5-82c3-fe3810fcaeff",
      "ff83e454-16f0-4b34-8e7c-1dcd09a93334",
      "5417e9c1-ed4f-4c7a-89c2-a1de25ca80ce",
      "fec00670-5952-499d-9bf7-d1a40c0fdfe2",
      "7304cd1b-3db2-4b3d-827e-cfa20331f1f1",
      "3889a527-1574-4ede-972e-79fa3af21ed0",
      "35456a67-4a6f-4cdd-ba96-21543b9c6179",
      "a080e2f7-6cff-4a25-a2d2-77999eabe083",
      "74be6da5-034f-472b-b68d-b34a79a8ab24",
      "27fcc5d6-4a55-42c0-abfc-fe1a42e5bd16",
      "afc67ec0-826f-43fd-80d9-31f3c863e702",
      "f2a8d08d-ec73-46fa-8545-d202e70913c0",
      "16741000-9730-4123-936d-7dc58180b32d",
      "a8918a73-eb27-45ea-afde-c41b507e605f"
    ],
    "created_by": "bb760e2d-d679-4b64-b2a9-03005b21870a",
    "created_time": 1550393450919,
    "id": "4ebeb41c-575b-4496-a31a-40e62860bf7b",
    "ignore_block_count": true,
    "last_edited_by": "bb760e2d-d679-4b64-b2a9-03005b21870a",
    "last_edited_time": 1550477220000,
    "parent_id": "8178616c-a0f2-4a08-93be-0454eaeffb70",
    "parent_table": "block",
    "properties": {
      "title": [
        [
          "Part 1 Tokenizing Input with Lex"
        ]
      ]
    },
    "type": "page",
    "version": 6,
    "content_resolved": [
      {
        "alive": true,
        "created_by": "bb760e2d-d679-4b64-b2a9-03005b21870a",
        "created_time": 1550393450914,
        "id": "d208e59c-2687-4956-b9f9-fc0e67c70860",
        "ignore_block_count": true,
        "last_edited_by": "bb760e2d-d679-4b64-b2a9-03005b21870a",
        "last_edited_time": 1550393450914,
        "parent_id": "4ebeb41c-575b-4496-a31a-40e62860bf7b",
        "parent_table": "block",
        "properties": {
          "title": [
            [
              "There are two steps that the code from example 1 carried out: one was "
            ],
            [
              "tokenizing",
              [
                [
                  "i"
                ]
              ]
            ],
            [
              " the input, which means it looked for symbols that constitute the arithmetic expression, and the second step was "
            ],
            [
              "parsing",
              [
                [
                  "i"
                ]
              ]
            ],
            [
              ", which involves analysing the extracted tokens and evaluating the result."
            ]
          ]
        },
        "type": "text",
        "version": 1,
        "inline_content": [
          {
            "Text": "There are two steps that the code from example 1 carried out: one was "
          },
          {
            "Text": "tokenizing",
            "AttrFlags": 4
          },
          {
            "Text": " the input, which means it looked for symbols that constitute the arithmetic expression, and the second step was "
          },
          {
            "Text": "parsing",
            "AttrFlags": 4
          },
          {
            "Text": ", which involves analysing the extracted tokens and evaluating the result."
          }
        ]
      },
      {
        "alive": true,
        "created_by": "bb760e2d-d679-4b64-b2a9-03005b21870a",
        "created_time": 1550393450916,
        "id": "617a0008-6f46-40f1-bb67-05ed38c83562",
        "ignore_block_count": true,
        "last_edited_by": "bb760e2d-d679-4b64-b2a9-03005b21870a",
        "last_edited_time": 1550393450916,
        "parent_id": "4ebeb41c-575b-4496-a31a-40e62860bf7b",
        "parent_table": "block",
        "properties": {
          "title": [
            [
              "This section provides a simple example of how to "
            ],
            [
              "tokenize",
              [
                [
                  "i"
                ]
              ]
            ],
            [
              " user input, and then breaks it down line by line."
            ]
          ]
        },
        "type": "text",
        "version": 1,
        "inline_content": [
          {
            "Text": "This section provides a simple example of how to "
          },
          {
            "Text": "tokenize",
            "AttrFlags": 4
          },
          {
            "Text": " user input, and then breaks it down line by line."
          }
        ]
      },
      {
        "alive": true,
        "created_by": "bb760e2d-d679-4b64-b2a9-03005b21870a",
        "created_time": 1550393450917,
        "id": "e507cf2f-024f-41b2-b072-1ad29c7102d8",
        "ignore_block_count": true,
        "last_edited_by": "bb760e2d-d679-4b64-b2a9-03005b21870a",
        "last_edited_time": 1550393450917,
        "parent_id": "4ebeb41c-575b-4496-a31a-40e62860bf7b",
        "parent_table": "block",
        "properties": {
          "language": [
            [
              "Plain Text"
            ]
          ],
          "title": [
            [
              "import ply.lex as lex\n\n# List of token names. This is always required\ntokens = [\n   'NUMBER',\n   'PLUS',\n   'MINUS',\n   'TIMES',\n   'DIVIDE',\n   'LPAREN',\n   'RPAREN',\n]\n\n# Regular expression rules for simple tokens\nt_PLUS    = r'\\+'\nt_MINUS   = r'-'\nt_TIMES   = r'\\*'\nt_DIVIDE  = r'/'\nt_LPAREN  = r'\\('\nt_RPAREN  = r'\\)'\n\n# A regular expression rule with some action code\ndef t_NUMBER(t):\n    r'\\d+'\n    t.value = int(t.value)    \n    return t\n\n# Define a rule so we can track line numbers\ndef t_newline(t):\n    r'\\n+'\n    t.lexer.lineno += len(t.value)\n\n# A string containing ignored characters (spaces and tabs)\nt_ignore  = ' \\t'\n\n# Error handling rule\ndef t_error(t):\n    print(\"Illegal character '%s'\" % t.value[0])\n    t.lexer.skip(1)\n\n# Build the lexer\nlexer = lex.lex()\n\n# Give the lexer some input\nlexer.input(data)\n\n# Tokenize\nwhile True:\n    tok = lexer.token()\n    if not tok: \n        break      # No more input\n    print(tok)"
            ]
          ]
        },
        "type": "code",
        "version": 1,
        "code": "import ply.lex as lex\n\n# List of token names. This is always required\ntokens = [\n   'NUMBER',\n   'PLUS',\n   'MINUS',\n   'TIMES',\n   'DIVIDE',\n   'LPAREN',\n   'RPAREN',\n]\n\n# Regular expression rules for simple tokens\nt_PLUS    = r'\\+'\nt_MINUS   = r'-'\nt_TIMES   = r'\\*'\nt_DIVIDE  = r'/'\nt_LPAREN  = r'\\('\nt_RPAREN  = r'\\)'\n\n# A regular expression rule with some action code\ndef t_NUMBER(t):\n    r'\\d+'\n    t.value = int(t.value)    \n    return t\n\n# Define a rule so we can track line numbers\ndef t_newline(t):\n    r'\\n+'\n    t.lexer.lineno += len(t.value)\n\n# A string containing ignored characters (spaces and tabs)\nt_ignore  = ' \\t'\n\n# Error handling rule\ndef t_error(t):\n    print(\"Illegal character '%s'\" % t.value[0])\n    t.lexer.skip(1)\n\n# Build the lexer\nlexer = lex.lex()\n\n# Give the lexer some input\nlexer.input(data)\n\n# Tokenize\nwhile True:\n    tok = lexer.token()\n    if not tok: \n        break      # No more input\n    print(tok)",
        "code_language": "Plain Text"
      },
      {
        "alive": true,
        "created_by": "bb760e2d-d679-4b64-b2a9-03005b21870a",
        "created_time": 1550393450917,
        "id": "a6ab81ef-230b-4fd2-9b13-d7e04afb2037",
        "ignore_block_count": true,
        "last_edited_by": "bb760e2d-d679-4b64-b2a9-03005b21870a",
        "last_edited_time": 1550393450917,
        "parent_id": "4ebeb41c-575b-4496-a31a-40e62860bf7b",
        "parent_table": "block",
        "properties": {
          "title": [
            [
              "Save this file as "
            ],
            [
              "calclex.py",
              [
                [
                  "c"
                ]
              ]
            ],
            [
              ". We’ll be using this when building our Yacc parser."
            ]
          ]
        },
        "type": "text",
        "version": 1,
        "inline_content": [
          {
            "Text": "Save this file as "
          },
          {
            "Text": "calclex.py",
            "AttrFlags": 2
          },
          {
            "Text": ". We’ll be using this when building our Yacc parser."
          }
        ]
      },
      {
        "alive": true,
        "created_by": "bb760e2d-d679-4b64-b2a9-03005b21870a",
        "created_time": 1550393450917,
        "id": "94d1f7d3-d744-4f2e-97da-1488e79d8269",
        "ignore_block_count": true,
        "last_edited_by": "bb760e2d-d679-4b64-b2a9-03005b21870a",
        "last_edited_time": 1550393450917,
        "parent_id": "4ebeb41c-575b-4496-a31a-40e62860bf7b",
        "parent_table": "block",
        "type": "divider",
        "version": 1
      },
      {
        "alive": true,
        "created_by": "bb760e2d-d679-4b64-b2a9-03005b21870a",
        "created_time": 1550393450917,
        "id": "30fdad2b-8a44-4887-9aed-fa2e00500592",
        "ignore_block_count": true,
        "last_edited_by": "bb760e2d-d679-4b64-b2a9-03005b21870a",
        "last_edited_time": 1550393450917,
        "parent_id": "4ebeb41c-575b-4496-a31a-40e62860bf7b",
        "parent_table": "block",
        "properties": {
          "title": [
            [
              "Breakdown"
            ]
          ]
        },
        "type": "header",
        "version": 1,
        "inline_content": [
          {
            "Text": "Breakdown"
          }
        ]
      },
      {
        "alive": true,
        "created_by": "bb760e2d-d679-4b64-b2a9-03005b21870a",
        "created_time": 1550393450917,
        "id": "7301d5e1-7de2-4db5-82c3-fe3810fcaeff",
        "ignore_block_count": true,
        "last_edited_by": "bb760e2d-d679-4b64-b2a9-03005b21870a",
        "last_edited_time": 1550393450917,
        "parent_id": "4ebeb41c-575b-4496-a31a-40e62860bf7b",
        "parent_table": "block",
        "properties": {
          "title": [
            [
              "Import the module using "
            ],
            [
              "import ply.lex",
              [
                [
                  "c"
                ]
              ]
            ]
          ]
        },
        "type": "numbered_list",
        "version": 1,
        "inline_content": [
          {
            "Text": "Import the module using "
          },
          {
            "Text": "import ply.lex",
            "AttrFlags": 2
          }
        ]
      },
      {
        "alive": true,
        "created_by": "bb760e2d-d679-4b64-b2a9-03005b21870a",
        "created_time": 1550393450918,
        "id": "ff83e454-16f0-4b34-8e7c-1dcd09a93334",
        "ignore_block_count": true,
        "last_edited_by": "bb760e2d-d679-4b64-b2a9-03005b21870a",
        "last_edited_time": 1550393450918,
        "parent_id": "4ebeb41c-575b-4496-a31a-40e62860bf7b",
        "parent_table": "block",
        "properties": {
          "title": [
            [
              "All lexers must provide a list called "
            ],
            [
              "tokens",
              [
                [
                  "c"
                ]
              ]
            ],
            [
              " that defines all of the possible token names that can be produced by the lexer. This list is always required."
            ]
          ]
        },
        "type": "numbered_list",
        "version": 1,
        "inline_content": [
          {
            "Text": "All lexers must provide a list called "
          },
          {
            "Text": "tokens",
            "AttrFlags": 2
          },
          {
            "Text": " that defines all of the possible token names that can be produced by the lexer. This list is always required."
          }
        ]
      },
      {
        "alive": true,
        "created_by": "bb760e2d-d679-4b64-b2a9-03005b21870a",
        "created_time": 1550393450918,
        "id": "5417e9c1-ed4f-4c7a-89c2-a1de25ca80ce",
        "ignore_block_count": true,
        "last_edited_by": "bb760e2d-d679-4b64-b2a9-03005b21870a",
        "last_edited_time": 1550393450918,
        "parent_id": "4ebeb41c-575b-4496-a31a-40e62860bf7b",
        "parent_table": "block",
        "properties": {
          "language": [
            [
              "Plain Text"
            ]
          ],
          "title": [
            [
              "tokens = [\n   'NUMBER',\n   'PLUS',\n   'MINUS',\n   'TIMES',\n   'DIVIDE',\n   'LPAREN',\n   'RPAREN',\n]"
            ]
          ]
        },
        "type": "code",
        "version": 1,
        "code": "tokens = [\n   'NUMBER',\n   'PLUS',\n   'MINUS',\n   'TIMES',\n   'DIVIDE',\n   'LPAREN',\n   'RPAREN',\n]",
        "code_language": "Plain Text"
      },
      {
        "alive": true,
        "created_by": "bb760e2d-d679-4b64-b2a9-03005b21870a",
        "created_time": 1550393450918,
        "id": "fec00670-5952-499d-9bf7-d1a40c0fdfe2",
        "ignore_block_count": true,
        "last_edited_by": "bb760e2d-d679-4b64-b2a9-03005b21870a",
        "last_edited_time": 1550393450918,
        "parent_id": "4ebeb41c-575b-4496-a31a-40e62860bf7b",
        "parent_table": "block",
        "properties": {
          "title": [
            [
              "tokens",
              [
                [
                  "c"
                ]
              ]
            ],
            [
              " could also be a tuple of strings (rather than a string), where each string denotes a token as before."
            ]
          ]
        },
        "type": "text",
        "version": 1,
        "inline_content": [
          {
            "Text": "tokens",
            "AttrFlags": 2
          },
          {
            "Text": " could also be a tuple of strings (rather than a string), where each string denotes a token as before."
          }
        ]
      },
      {
        "alive": true,
        "created_by": "bb760e2d-d679-4b64-b2a9-03005b21870a",
        "created_time": 1550393450918,
        "id": "7304cd1b-3db2-4b3d-827e-cfa20331f1f1",
        "ignore_block_count": true,
        "last_edited_by": "bb760e2d-d679-4b64-b2a9-03005b21870a",
        "last_edited_time": 1550393450918,
        "parent_id": "4ebeb41c-575b-4496-a31a-40e62860bf7b",
        "parent_table": "block",
        "properties": {
          "title": [
            [
              "The regex rule for each string may be defined either as a string or as a function. In either case, the variable name should be prefixed by t_ to denote it is a rule for matching tokens."
            ]
          ]
        },
        "type": "numbered_list",
        "version": 1,
        "inline_content": [
          {
            "Text": "The regex rule for each string may be defined either as a string or as a function. In either case, the variable name should be prefixed by t_ to denote it is a rule for matching tokens."
          }
        ]
      },
      {
        "alive": true,
        "created_by": "bb760e2d-d679-4b64-b2a9-03005b21870a",
        "created_time": 1550393450918,
        "id": "3889a527-1574-4ede-972e-79fa3af21ed0",
        "ignore_block_count": true,
        "last_edited_by": "bb760e2d-d679-4b64-b2a9-03005b21870a",
        "last_edited_time": 1550393450918,
        "parent_id": "4ebeb41c-575b-4496-a31a-40e62860bf7b",
        "parent_table": "block",
        "properties": {
          "title": [
            [
              "For simple tokens, the regular expression can be specified as strings: "
            ],
            [
              "t_PLUS = r'\\+'",
              [
                [
                  "c"
                ]
              ]
            ]
          ]
        },
        "type": "bulleted_list",
        "version": 1,
        "inline_content": [
          {
            "Text": "For simple tokens, the regular expression can be specified as strings: "
          },
          {
            "Text": "t_PLUS = r'\\+'",
            "AttrFlags": 2
          }
        ]
      },
      {
        "alive": true,
        "created_by": "bb760e2d-d679-4b64-b2a9-03005b21870a",
        "created_time": 1550393450918,
        "id": "35456a67-4a6f-4cdd-ba96-21543b9c6179",
        "ignore_block_count": true,
        "last_edited_by": "bb760e2d-d679-4b64-b2a9-03005b21870a",
        "last_edited_time": 1550393450918,
        "parent_id": "4ebeb41c-575b-4496-a31a-40e62860bf7b",
        "parent_table": "block",
        "properties": {
          "title": [
            [
              "If some kind of action needs to be performed, a token rule can be specified as a function."
            ]
          ]
        },
        "type": "bulleted_list",
        "version": 1,
        "inline_content": [
          {
            "Text": "If some kind of action needs to be performed, a token rule can be specified as a function."
          }
        ]
      },
      {
        "alive": true,
        "created_by": "bb760e2d-d679-4b64-b2a9-03005b21870a",
        "created_time": 1550393450918,
        "id": "a080e2f7-6cff-4a25-a2d2-77999eabe083",
        "ignore_block_count": true,
        "last_edited_by": "bb760e2d-d679-4b64-b2a9-03005b21870a",
        "last_edited_time": 1550393450918,
        "parent_id": "4ebeb41c-575b-4496-a31a-40e62860bf7b",
        "parent_table": "block",
        "properties": {
          "language": [
            [
              "Plain Text"
            ]
          ],
          "title": [
            [
              "def t_NUMBER(t):\n    r'\\d+'\n    t.value = int(t.value)\n    return t\nNote, the rule is specified as a doc string within the function. The function accepts one argument which is an instance of `LexToken`, performs some action and then returns back the argument. \n\nIf you want to use an external string as the regex rule for the function instead of specifying a doc string, consider the following example:\n\n@TOKEN(identifier)         # identifier is a string holding the regex\ndef t_ID(t):\n    ...      # actions\n\n* An instance of `LexToken` object (let's call this object `t`) has the following attributes:\n1) `t.type` which is the token type (as a string) (eg: `'NUMBER'`, `'PLUS'`, etc). By default, `t.type` is set to the name following the `t_` prefix.\n2) `t.value` which is the lexeme (the actual text matched) \n3) `t.lineno` which is the current line number (this is not automatically updated, as the lexer knows nothing of line numbers). Update lineno using a function called `t_newline`.\n\n## \n\ndef t_newline(t):\n    r'\\n+'\n    t.lexer.lineno += len(t.value)\n\n##\n4) `t.lexpos` which is the position of the token relative to the beginning of the input text."
            ]
          ]
        },
        "type": "code",
        "version": 1,
        "code": "def t_NUMBER(t):\n    r'\\d+'\n    t.value = int(t.value)\n    return t\nNote, the rule is specified as a doc string within the function. The function accepts one argument which is an instance of `LexToken`, performs some action and then returns back the argument. \n\nIf you want to use an external string as the regex rule for the function instead of specifying a doc string, consider the following example:\n\n@TOKEN(identifier)         # identifier is a string holding the regex\ndef t_ID(t):\n    ...      # actions\n\n* An instance of `LexToken` object (let's call this object `t`) has the following attributes:\n1) `t.type` which is the token type (as a string) (eg: `'NUMBER'`, `'PLUS'`, etc). By default, `t.type` is set to the name following the `t_` prefix.\n2) `t.value` which is the lexeme (the actual text matched) \n3) `t.lineno` which is the current line number (this is not automatically updated, as the lexer knows nothing of line numbers). Update lineno using a function called `t_newline`.\n\n## \n\ndef t_newline(t):\n    r'\\n+'\n    t.lexer.lineno += len(t.value)\n\n##\n4) `t.lexpos` which is the position of the token relative to the beginning of the input text.",
        "code_language": "Plain Text"
      },
      {
        "alive": true,
        "created_by": "bb760e2d-d679-4b64-b2a9-03005b21870a",
        "created_time": 1550393450919,
        "id": "74be6da5-034f-472b-b68d-b34a79a8ab24",
        "ignore_block_count": true,
        "last_edited_by": "bb760e2d-d679-4b64-b2a9-03005b21870a",
        "last_edited_time": 1550393450919,
        "parent_id": "4ebeb41c-575b-4496-a31a-40e62860bf7b",
        "parent_table": "block",
        "properties": {
          "title": [
            [
              "If nothing is returned from a regex rule function, the token is discarded. If you want to discard a token, you can alternatively add t_ignore_ prefix to a regex rule variable instead of defining a function for the same rule."
            ]
          ]
        },
        "type": "bulleted_list",
        "version": 1,
        "inline_content": [
          {
            "Text": "If nothing is returned from a regex rule function, the token is discarded. If you want to discard a token, you can alternatively add t_ignore_ prefix to a regex rule variable instead of defining a function for the same rule."
          }
        ]
      },
      {
        "alive": true,
        "created_by": "bb760e2d-d679-4b64-b2a9-03005b21870a",
        "created_time": 1550393450919,
        "id": "27fcc5d6-4a55-42c0-abfc-fe1a42e5bd16",
        "ignore_block_count": true,
        "last_edited_by": "bb760e2d-d679-4b64-b2a9-03005b21870a",
        "last_edited_time": 1550393450919,
        "parent_id": "4ebeb41c-575b-4496-a31a-40e62860bf7b",
        "parent_table": "block",
        "properties": {
          "language": [
            [
              "Plain Text"
            ]
          ],
          "title": [
            [
              "def t_COMMENT(t):\n    r'\\#.*'\n    pass\n    # No return value. Token discarded\n\n...Is the same as:\n\nt_ignore_COMMENT = r'\\#.*'\n##\n\n\u003csup\u003eThis is of course invalid if you're carrying out some action when you see a comment. In which case, use a function to define the regex rule.\u003c/sup\u003e \n\nIf you haven't defined a token for some characters but still want to ignore it, use `t_ignore = \"\u003ccharacters to ignore\u003e\"` (these prefixes are necessary):          \n\nt_ignore_COMMENT = r'\\#.*'\nt_ignore  = ' \\t'    # ignores spaces and tabs\n\n##\n- When building the master regex, lex will add the regexes specified in the file as follows: \n1) Tokens defined by functions are added in the same order as they appear in the file. \n2) Tokens defined by strings are added in decreasing order of the string length of the string defining the regex for that token.\n\nIf you are matching `==` and `=` in the same file, take advantage of these rules.\n##\n- Literals are tokens that are returned as they are. Both `t.type` and `t.value` will be set to the character itself."
            ]
          ]
        },
        "type": "code",
        "version": 1,
        "code": "def t_COMMENT(t):\n    r'\\#.*'\n    pass\n    # No return value. Token discarded\n\n...Is the same as:\n\nt_ignore_COMMENT = r'\\#.*'\n##\n\n\u003csup\u003eThis is of course invalid if you're carrying out some action when you see a comment. In which case, use a function to define the regex rule.\u003c/sup\u003e \n\nIf you haven't defined a token for some characters but still want to ignore it, use `t_ignore = \"\u003ccharacters to ignore\u003e\"` (these prefixes are necessary):          \n\nt_ignore_COMMENT = r'\\#.*'\nt_ignore  = ' \\t'    # ignores spaces and tabs\n\n##\n- When building the master regex, lex will add the regexes specified in the file as follows: \n1) Tokens defined by functions are added in the same order as they appear in the file. \n2) Tokens defined by strings are added in decreasing order of the string length of the string defining the regex for that token.\n\nIf you are matching `==` and `=` in the same file, take advantage of these rules.\n##\n- Literals are tokens that are returned as they are. Both `t.type` and `t.value` will be set to the character itself.",
        "code_language": "Plain Text"
      },
      {
        "alive": true,
        "created_by": "bb760e2d-d679-4b64-b2a9-03005b21870a",
        "created_time": 1550393450919,
        "id": "afc67ec0-826f-43fd-80d9-31f3c863e702",
        "ignore_block_count": true,
        "last_edited_by": "bb760e2d-d679-4b64-b2a9-03005b21870a",
        "last_edited_time": 1550393450919,
        "parent_id": "4ebeb41c-575b-4496-a31a-40e62860bf7b",
        "parent_table": "block",
        "properties": {
          "title": [
            [
              "Define a list of literals as such:"
            ]
          ]
        },
        "type": "text",
        "version": 1,
        "inline_content": [
          {
            "Text": "Define a list of literals as such:"
          }
        ]
      },
      {
        "alive": true,
        "created_by": "bb760e2d-d679-4b64-b2a9-03005b21870a",
        "created_time": 1550393450919,
        "id": "f2a8d08d-ec73-46fa-8545-d202e70913c0",
        "ignore_block_count": true,
        "last_edited_by": "bb760e2d-d679-4b64-b2a9-03005b21870a",
        "last_edited_time": 1550393450919,
        "parent_id": "4ebeb41c-575b-4496-a31a-40e62860bf7b",
        "parent_table": "block",
        "properties": {
          "language": [
            [
              "Plain Text"
            ]
          ],
          "title": [
            [
              "literals = [ '+', '-', '*', '/' ]\n\nor,\n\nliterals = \"+-*/\"\n\n##\n\nIt is possible to write token functions that perform additional actions when literals are matched. However, you'll need to set the token type appropriately. For example:\n\nliterals = [ '{', '}' ]\n\ndef t_lbrace(t):\n    r'\\{'\n    t.type = '{'  # Set token type to the expected literal (ABSOLUTE MUST if this is a literal)\n    return t\n\n##\n- Handle errors with t_error function.\n\n# Error handling rule\ndef t_error(t):\n  print(\"Illegal character '%s'\" % t.value[0])\n  t.lexer.skip(1) # skip the illegal token (don't process it)\n\nIn general, `t.lexer.skip(n)` skips n characters in the input string."
            ]
          ]
        },
        "type": "code",
        "version": 1,
        "code": "literals = [ '+', '-', '*', '/' ]\n\nor,\n\nliterals = \"+-*/\"\n\n##\n\nIt is possible to write token functions that perform additional actions when literals are matched. However, you'll need to set the token type appropriately. For example:\n\nliterals = [ '{', '}' ]\n\ndef t_lbrace(t):\n    r'\\{'\n    t.type = '{'  # Set token type to the expected literal (ABSOLUTE MUST if this is a literal)\n    return t\n\n##\n- Handle errors with t_error function.\n\n# Error handling rule\ndef t_error(t):\n  print(\"Illegal character '%s'\" % t.value[0])\n  t.lexer.skip(1) # skip the illegal token (don't process it)\n\nIn general, `t.lexer.skip(n)` skips n characters in the input string.",
        "code_language": "Plain Text"
      },
      {
        "alive": true,
        "created_by": "bb760e2d-d679-4b64-b2a9-03005b21870a",
        "created_time": 1550393450919,
        "id": "16741000-9730-4123-936d-7dc58180b32d",
        "ignore_block_count": true,
        "last_edited_by": "bb760e2d-d679-4b64-b2a9-03005b21870a",
        "last_edited_time": 1550393450919,
        "parent_id": "4ebeb41c-575b-4496-a31a-40e62860bf7b",
        "parent_table": "block",
        "properties": {
          "title": [
            [
              "Final preparations:"
            ]
          ]
        },
        "type": "numbered_list",
        "version": 1,
        "inline_content": [
          {
            "Text": "Final preparations:"
          }
        ]
      },
      {
        "alive": true,
        "created_by": "bb760e2d-d679-4b64-b2a9-03005b21870a",
        "created_time": 1550393450919,
        "id": "a8918a73-eb27-45ea-afde-c41b507e605f",
        "ignore_block_count": true,
        "last_edited_by": "bb760e2d-d679-4b64-b2a9-03005b21870a",
        "last_edited_time": 1550393450919,
        "parent_id": "4ebeb41c-575b-4496-a31a-40e62860bf7b",
        "parent_table": "block",
        "properties": {
          "language": [
            [
              "Plain Text"
            ]
          ],
          "title": [
            [
              "Build the lexer using `lexer = lex.lex()`.\n\n##\n You can also put everything inside a class and call use instance of the class to define the lexer. Eg:\n\n##  \n   import ply.lex as lex  \n   class MyLexer(object):            \n         ...     # everything relating to token rules and error handling comes here as usual \n\n         # Build the lexer\n         def build(self, **kwargs):\n             self.lexer = lex.lex(module=self, **kwargs)\n\n         def test(self, data):\n             self.lexer.input(data)\n             for token in self.lexer.token():\n                 print(token)\n\n         # Build the lexer and try it out\n\n   m = MyLexer()\n   m.build()           # Build the lexer\n   m.test(\"3 + 4\")     #\n\nProvide input using `lexer.input(data)` where data is a string \n\nTo get the tokens, use `lexer.token()` which returns tokens matched. You can iterate over lexer in a loop as in:\n\n##\n\n  for i in lexer: \n      print(i)"
            ]
          ]
        },
        "type": "code",
        "version": 1,
        "code": "Build the lexer using `lexer = lex.lex()`.\n\n##\n You can also put everything inside a class and call use instance of the class to define the lexer. Eg:\n\n##  \n   import ply.lex as lex  \n   class MyLexer(object):            \n         ...     # everything relating to token rules and error handling comes here as usual \n\n         # Build the lexer\n         def build(self, **kwargs):\n             self.lexer = lex.lex(module=self, **kwargs)\n\n         def test(self, data):\n             self.lexer.input(data)\n             for token in self.lexer.token():\n                 print(token)\n\n         # Build the lexer and try it out\n\n   m = MyLexer()\n   m.build()           # Build the lexer\n   m.test(\"3 + 4\")     #\n\nProvide input using `lexer.input(data)` where data is a string \n\nTo get the tokens, use `lexer.token()` which returns tokens matched. You can iterate over lexer in a loop as in:\n\n##\n\n  for i in lexer: \n      print(i)",
        "code_language": "Plain Text"
      }
    ],
    "title": "Part 1 Tokenizing Input with Lex"
  },
  "Users": [
    {
      "email": "kkowalczyk@gmail.com",
      "family_name": "Kowalczyk",
      "given_name": "Krzysztof",
      "id": "bb760e2d-d679-4b64-b2a9-03005b21870a",
      "locale": "en",
      "mobile_onboarding_completed": true,
      "onboarding_completed": true,
      "profile_photo": "https://s3-us-west-2.amazonaws.com/public.notion-static.com/2dcaa66c-7674-4ff6-9924-601785b63561/head-bw-640x960.png",
      "time_zone": "America/Los_Angeles",
      "version": 18
    }
  ],
  "Tables": null
}